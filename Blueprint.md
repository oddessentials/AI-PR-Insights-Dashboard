# **üìò PART 1 DELIVERABLE ‚Äî AI Knowledge Worker POC**

_(Lean, Fact-Focused, Ready for MBA Reviewer)_

## **0. Context (Optional Intro Section)**

This Part 1 deliverable extends the previous market research and product plan (AI-Powered Azure DevOps PR Analyzer) by introducing a **Working Proof of Concept (POC)** for an **AI Knowledge Worker**.
The AI Knowledge Worker operates on top of an existing Python-based PR aggregation system capable of extracting, normalizing, and storing historical Pull Request data across an entire Azure DevOps organization.

The POC showcases how LLM-based reasoning and retrieval-augmented analysis can provide **summaries, quality checks, insights, and executive narratives** about both **human and AI-generated** contributions, without favoring one over the other.

---

# **A. Basic Knowledge Worker Setup (50 pts)**

### **A.1. Purpose of the Knowledge Worker**

The AI Knowledge Worker is designed to **analyze code-review activity across Azure DevOps**, synthesizing insights that humans cannot efficiently derive manually.
It focuses on:

- PR summaries
- Review quality insights
- Contributor activity
- Cross-repo patterns
- Team participation
- Risk identification
- Balanced contribution across human collaborators
- Quality gates for AI agent contributions

### **A.2. Role Definition**

**‚ÄúSenior Engineering Intelligence Analyst‚Äù**
A domain-specialized agent responsible for ingesting:

- code diffs
- metadata
- review threads
- historical PR activity
- developer contribution patterns
- AI-generated PR signals
  and producing:
- structured summaries
- descriptive and prescriptive insights
- flags for anomalies
- team-level narratives for executives

### **A.3. Technical Setup**

- LLM core (GPT-4.1, GPT-4 Turbo, or similar)
- Embedded system prompt defining task scope, style, safety
- RAG layer built on:

  - the user's own database of historical PRs
  - embeddings or keyword retrieval

- Structured JSON output format for deterministic consumption
- Optional scoring block (risk score, anomaly score, summary confidence)

### **A.4. Capabilities Implemented in POC**

1. **Pull Request Summarization**
2. **Change Impact Reasoning**
3. **Reviewer Load Insights**
4. **Contributor Trend Analysis**
5. **Team Participation Health**
6. **AI-vs-Human Contribution Signals (optional)**
7. **Narrative Generation for Exec Dashboards**
8. **Cross-Repo Pattern Detection**

All of these derive entirely from your aggregated PR dataset + LLM analysis.

---

# **B. Additional Sources of Context (25 pts)**

Your existing PR aggregation solves the most difficult part:
‚úî extracting PRs via ADO APIs
‚úî normalizing schema
‚úî maintaining a historical DB snapshot

The knowledge worker enriches this with additional context:

### **B.1. Historical PR Metadata**

- timestamps
- authors
- reviewers
- merge policies
- approvals
- review comment density
- cycle time metrics
- repository ownership patterns

### **B.2. Code Diff Snippets (tokens or compressed)**

LLM evaluates:

- novelty
- risk
- potential regressions
- adherence to project standards

### **B.3. AI-Origin Metadata (Optional, Non-Judgmental)**

The system does _not_ rank AI vs. human PRs.
Instead, it labels PRs with contextual attributes to enhance analysis:

- AI-generated commit signatures
- high-volume autogenerated code
- unusual structural similarity
- style drift relative to baseline patterns

### **B.4. Organizational Context**

- which team authored the PR
- department alignment
- cross-team review participation rates
- relative contribution share per quarter

### **B.5. Executive Context**

- mapping PR activity to DORA-related outcomes
- cycle time-increase explanations
- bottleneck diagnosis

### **B.6. Embeddings / Vector Context (optional)**

- Similar PR clusters
- Behavioral trends
- Outlier detection based on embeddings

---

# **C. Agent Evaluation Setup (25 pts)**

### **C.1. Evaluation Queries Used in POC**

Each evaluation query is a high-value question the agent should reliably answer:

1. ‚ÄúSummarize the purpose and impact of this PR.‚Äù
2. ‚ÄúIdentify risks, missing tests, or unexpected patterns.‚Äù
3. ‚ÄúExplain how this PR aligns with historical patterns for this repo.‚Äù
4. ‚ÄúCompare reviewer participation to team norms.‚Äù
5. ‚ÄúIdentify if contributor workload is imbalanced.‚Äù
6. ‚ÄúExplain anomalies or delays exceeding baseline cycle time.‚Äù
7. ‚ÄúDetect AI-style errors (hallucinations, inconsistent naming, missing checks).‚Äù
8. ‚ÄúProduce an executive-ready narrative of review health for the quarter.‚Äù

### **C.2. Scoring Model**

Each response is evaluated on:

- **Accuracy**
- **Completeness**
- **Factual grounding in retrieved context**
- **Hallucination resistance**
- **Relevance to PR**
- **Actionable insight quality**

### **C.3. Benchmarking**

Against:

- Human-written summaries
- Known PR characteristics (metadata)
- Statistical norms across repo/org
- Known risk patterns

### **C.4. Pass/Fail Criteria**

A response ‚Äúpasses‚Äù if:

- It extracts correct metadata
- It summarizes the diff without hallucination
- It identifies at least one relevant review insight
- It avoids overclaiming (‚Äúcannot be determined from context‚Äù)
- It expresses confidence levels when needed

---

# **D. Proper Guardrails for Safety & Accuracy (25 pts)**

### **D.1. Hallucination Reduction**

- Strict ‚Äúdon‚Äôt guess‚Äù behavior
- Respond with ‚ÄúNot present in context‚Äù when uncertain
- Only use retrieved DB context + PR data

### **D.2. Code Safety Policies**

- No insecure suggestions
- No speculative vulnerabilities
- No generation of entire modules unless explicitly allowed

### **D.3. Privacy & PI Assurance**

Especially relevant since your org is healthcare insurance:

- Never infer PHI
- No team-member evaluations identifying personal performance weaknesses
- Only aggregate-level insights

### **D.4. Organizational Boundaries**

- No access to private repos outside the authenticated scope
- No rewriting of protected files

### **D.5. AI-vs-Human Neutrality**

The guardrail explicitly forbids value judgments:

> ‚ÄúDo not declare whether AI or humans perform better. Treat both equally and objectively.‚Äù

### **D.6. Deterministic Output**

- JSON schema validation
- Structured results for dashboards

---

# **E. Prompt & Instruction Quality (25 pts)**

### **E.1. System Prompt Core**

Defines the agent as:

- neutral
- fact-based
- structured
- data-driven

### **E.2. Retrieval-Aware Prompting**

Includes:

- context injection
- chain-of-thought suppression
- visible reasoning restricted to short bullet points
- explicit refusal policy

### **E.3. Few-Shot Examples**

- Example PR summaries
- Example risk analyses
- Example reviewer load explanations

### **E.4. Output Structure**

- Summary
- Risks
- Reviewer insights
- Contribution patterns
- Exec narrative
- Confidence score

### **E.5. Metadata-Driven Behavior**

The agent reacts conditionally based on:

- repo history
- contributor frequency
- cycle time baselines

---

# **F. Final POC Write-Up & Presentation (50 pts)**

### **F.1. Narrative of the POC**

This POC demonstrates how an AI Knowledge Worker can operate across an entire Azure DevOps environment using your previously developed PR aggregation engine.

### **F.2. Screenshots / UI**

You may reuse:

- The GitHub Pages dashboard
- Power BI mock cards
- Figma-like diagrams

### **F.3. User Workflows Demonstrated**

1. Manager asks: ‚ÄúWhy did cycle time increase last month?‚Äù
2. AI Knowledge Worker generates narrative + chart context.
3. Engineer asks: ‚ÄúSummarize PR #4321 and identify potential issues.‚Äù
4. Agent produces structured summary and flags.
5. Director asks: ‚ÄúAre reviewers overloaded?‚Äù
6. Agent identifies imbalances and suggests rotation improvements.
7. Org leader asks: ‚ÄúHow does AI agent contribution compare to human contribution?‚Äù
8. Agent produces descriptive, neutral metrics without value judgment.

### **F.4. Validation of Product-Market Fit**

- Human review workflows are overloaded.
- Executives require narratives, not dashboards.
- AI-generated PRs introduce unique signals requiring augmented evaluation.

### **F.5. Future Work**

- API-based automation
- Automated reviewer assignment
- Expanded anomaly detection
- PR-level forecasting

---

# ‚úîÔ∏è Summary (the plan in one sentence)

This Part 1 POC demonstrates an LLM-powered **RAG-based engineering analyst** that synthesizes your historical Azure DevOps pull request dataset into rich summaries, insights, risk analyses, and executive narratives‚Äîequally applicable to human and AI-generated contributions‚Äîwith neutral handling, clear guardrails, structured evaluation, and an extendable pipeline for future productization.
